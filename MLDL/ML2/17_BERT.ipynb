{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"17_BERT.ipynb","provenance":[{"file_id":"1oFNGv8rpK4rTQerlk75AFtRMNdpzKMiE","timestamp":1595421640718},{"file_id":"14QMN8BstnVcnXcDPYbbvMleBCRPfRijT","timestamp":1595415273791}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"qV40U7mLZkN4","colab_type":"text"},"source":["# Tensorflow 실습 : BERT 모델 이용하기"]},{"cell_type":"code","metadata":{"id":"dVwJbK-au9hE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":221},"executionInfo":{"status":"ok","timestamp":1596177181248,"user_tz":-540,"elapsed":57593,"user":{"displayName":"이예준","photoUrl":"","userId":"00427163814690958255"}},"outputId":"62599070-4ddc-4529-a127-3e86f8cdc478"},"source":["# bert 관련 import를 위해 설치\n","!pip install -q tf-models-nightly"],"execution_count":1,"outputs":[{"output_type":"stream","text":["\u001b[K     |████████████████████████████████| 870kB 5.9MB/s \n","\u001b[K     |████████████████████████████████| 325.2MB 46kB/s \n","\u001b[K     |████████████████████████████████| 102kB 7.9MB/s \n","\u001b[K     |████████████████████████████████| 174kB 49.7MB/s \n","\u001b[K     |████████████████████████████████| 36.4MB 119kB/s \n","\u001b[K     |████████████████████████████████| 358kB 41.2MB/s \n","\u001b[K     |████████████████████████████████| 1.1MB 48.7MB/s \n","\u001b[K     |████████████████████████████████| 6.7MB 49.5MB/s \n","\u001b[K     |████████████████████████████████| 460kB 52.3MB/s \n","\u001b[K     |████████████████████████████████| 296kB 49.1MB/s \n","\u001b[?25h  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vklYzrmMMfS9","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596177182604,"user_tz":-540,"elapsed":58940,"user":{"displayName":"이예준","photoUrl":"","userId":"00427163814690958255"}}},"source":["import tensorflow as tf\n","import tensorflow_hub as hub\n","import official.nlp.bert.tokenization"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VxKHYpSJNJpN","colab_type":"text"},"source":["## BERT 모델 \n","- 기본적으로 [BERT](https://arxiv.org/pdf/1810.04805.pdf)(Bidirectional Encoder Representations from Transformers)는 Transformer Encoder의 네트워크 구조를 가지고 있음\n","  - input shape : batch, sequence length (정수 token sequence)\n","  - output shape : batch, sequence length, feature\n","  - pre-trained 모델을 다양한 task의 데이터셋에 fine-tuning 했을 때 좋은 성능을 보임\n","  - pre-training 방법\n","    - Masked Language Model (MLM): token의 일부를 가리고, 어떤 token이 들어갈지 맞추는 문제로 학습\n","    - Next Sentence Prediction (NSP): 두 개의 문장을 넣어주고, 두 문장이 이어지는 문장인지 아닌지 맞추는 문제로 학습\n","    \n","\n","\n","<table>\n","  <tr><td>\n","    <img src=\"https://mino-park7.github.io/images/2018/12/%EA%B7%B8%EB%A6%BC1-bert-openai-gpt-elmo-%EC%B6%9C%EC%B2%98-bert%EB%85%BC%EB%AC%B8.png\" width=\"800\">\n","  </td></tr>\n","  <tr><td align=\"center\">\n","    <b>그림.</b> BERT의 네트워크 구조 (Encoder 구조라서 Bidirectional의 화살표를 모두 가지고 있음) <br/>&nbsp;\n","  </td></tr>\n","</table>\n","\n","\n","<table>\n","  <tr><td>\n","    <img src=\"https://raw.githubusercontent.com/jiabaogithub/imgs/master/img/bert20190926153602.png\" width=\"800\">\n","  </td></tr>\n","  <tr><td align=\"center\">\n","    <b>그림.</b> BERT의 Pre-training과 Fine-Tuning <br/>&nbsp;\n","  </td></tr>\n","</table>\n","\n","\n","<table>\n","  <tr><td>\n","    <img src=\"https://mino-park7.github.io/images/2019/02/%EA%B7%B8%EB%A6%BC4-bert-experiment-result.png\" width=\"800\">\n","  </td></tr>\n","  <tr><td align=\"center\">\n","    <b>그림.</b> BERT의 Fine-Tuning <br/>&nbsp;\n","  </td></tr>\n","</table>\n"]},{"cell_type":"markdown","metadata":{"id":"d3LzTrDpi8rG","colab_type":"text"},"source":["### BERT (Multilingual)\n","- [BERT (Multilingual)](https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/2)는 한국어, 영어 뿐 아니라 수많은 언어를 지원하는 BERT 모델\n","  - Multilingual Wikipedia dataset으로 학습이 된 모델 (pre-trained)\n","  - 한국어 전용 모델이 아니기 때문에 tokenizer의 성능이 좋지 않음\n","  - 한국어 전용 모델보다는 성능이 떨어지지만, 성능 자체는 나쁘지 않음\n","  - 해당 공개 모델에 대한 소개 [github](https://github.com/google-research/bert/blob/master/multilingual.md)\n","  - keras_hub의 multilingual 공개 모델: https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/2\n","\n","- 이외의 한국어 BERT 모델 (pre-trained)\n","  - [KoBERT](https://github.com/SKTBrain/KoBERT)\n","  - [KorBERT](http://aiopen.etri.re.kr/service_dataset.php)\n","  - [DistilKoBERT](https://github.com/monologg/DistilKoBERT)\n","  - [KoGPT-2](https://github.com/SKT-AI/KoGPT2) (BERT는 아니지만, 참고)\n"]},{"cell_type":"code","metadata":{"id":"OBmXucCdQCvw","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596177197553,"user_tz":-540,"elapsed":73884,"user":{"displayName":"이예준","photoUrl":"","userId":"00427163814690958255"}}},"source":["hub_url_bert = \"https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/2\"\n","bert_layer = hub.KerasLayer(hub_url_bert, trainable=True)"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fhzeWPMWTt09","colab_type":"text"},"source":["- 최대 sequence 길이는 자유롭게 선택\n","  - 길이가 길면 GPU 메모리 error\n","- GPU 메모리 해결법\n","  - 최대 길이 줄이기\n","  - mini batch size 줄이기"]},{"cell_type":"code","metadata":{"id":"XmYuKNettY6u","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596177197555,"user_tz":-540,"elapsed":73881,"user":{"displayName":"이예준","photoUrl":"","userId":"00427163814690958255"}}},"source":["# max_seq_length = 128  # Your choice here.\n","max_seq_length = 64\n","input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"input_word_ids\")\n","input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"input_mask\")\n","segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"segment_ids\")"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zsXV_SFmW_ly","colab_type":"text"},"source":["- input으로 출력 결과 shape 확인\n","  - pooled_output: 모든 sequence에 대한 output state를 시간 축으로 pooling해서 출력\n","  - sequence_output: 각 time step에 대한 hidden state 출력 "]},{"cell_type":"code","metadata":{"id":"rUdrOXinMg-m","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1596177198511,"user_tz":-540,"elapsed":74814,"user":{"displayName":"이예준","photoUrl":"","userId":"00427163814690958255"}},"outputId":"e3a576dd-52cc-4fc0-ad93-c1cf32c7b8e0"},"source":["pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n","print(pooled_output)\n","print(sequence_output)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Tensor(\"keras_layer/cond/Identity:0\", shape=(None, 768), dtype=float32)\n","Tensor(\"keras_layer/cond/Identity_1:0\", shape=(None, 64, 768), dtype=float32)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1QEoGjYHXWfU","colab_type":"text"},"source":["## 텍스트 전처리"]},{"cell_type":"markdown","metadata":{"id":"G1TpCt2QXcz6","colab_type":"text"},"source":["### Tokenizer\n","- multilingual model이므로 vocab size가 굉장히 큼"]},{"cell_type":"code","metadata":{"id":"GUiUPfCyt4mP","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596177199402,"user_tz":-540,"elapsed":75701,"user":{"displayName":"이예준","photoUrl":"","userId":"00427163814690958255"}}},"source":["# 저장된 tokenizer 불러오기\n","vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n","do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n","tokenizer = official.nlp.bert.tokenization.FullTokenizer(vocab_file, do_lower_case)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"_sgrA328wUcu","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1596177199403,"user_tz":-540,"elapsed":75688,"user":{"displayName":"이예준","photoUrl":"","userId":"00427163814690958255"}},"outputId":"dc811ae5-139b-47b7-fdff-adaa0d558fe8"},"source":["print(\"Vocab size:\", len(tokenizer.vocab))"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Vocab size: 119547\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ANQh5QReKY7Z","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1596177199404,"user_tz":-540,"elapsed":75679,"user":{"displayName":"이예준","photoUrl":"","userId":"00427163814690958255"}},"outputId":"bfb86a88-51d9-4950-c180-ef29b639fb26"},"source":["tokens = tokenizer.tokenize('english랑 한국어를 동시에 tokenize 해준다니!')\n","print(tokens)\n","ids = tokenizer.convert_tokens_to_ids(tokens)\n","print(ids)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["['engl', '##ish', '##랑', '한국', '##어를', '동시에', 'tok', '##eni', '##ze', '해', '##준', '##다', '##니', '!']\n","[20207, 15529, 62200, 48556, 80940, 58248, 18436, 18687, 10870, 9960, 54867, 11903, 25503, 106]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TIM9gK6BKpRW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1596177199405,"user_tz":-540,"elapsed":75648,"user":{"displayName":"이예준","photoUrl":"","userId":"00427163814690958255"}},"outputId":"d7fcb014-bae6-48a5-ac94-521248516a4c"},"source":["print(tokenizer.convert_ids_to_tokens(ids))"],"execution_count":9,"outputs":[{"output_type":"stream","text":["['engl', '##ish', '##랑', '한국', '##어를', '동시에', 'tok', '##eni', '##ze', '해', '##준', '##다', '##니', '!']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Q3DNTgNQ7le2","colab_type":"text"},"source":["- `[CLS]`는 classification token \n","\n","=> **(중요) classification하기 위한 dense layer를 [CLS] token의 output hidden state에 계산하여 사용함**\n","\n","- `[SEP]`는 문장 구분 token \n","  - 기본적으로 BERT는 2개 문장이 input으로 들어온다고 가정함\n","- `[PAD]`는 의미 없는 뒷 부분 zero padding에 사용하는 token\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"gFG64AYRxBFT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1596177199407,"user_tz":-540,"elapsed":75616,"user":{"displayName":"이예준","photoUrl":"","userId":"00427163814690958255"}},"outputId":"b5fdcf28-5d07-456c-ff5e-e2ac4fcb357d"},"source":["tokenizer.convert_tokens_to_ids(['[CLS]', '[SEP]', '[PAD]'])"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[101, 102, 0]"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"r-71RaXKd6cw","colab_type":"text"},"source":["### BERT input 형식에 맞게 변환"]},{"cell_type":"markdown","metadata":{"id":"hs6Dc64kYE3t","colab_type":"text"},"source":["<table>\n","  <tr><td>\n","    <img src=\"https://www.lyrn.ai/wp-content/uploads/2018/11/NSP.png\" width=\"800\">\n","  </td></tr>\n","  <tr><td align=\"center\">\n","    <b>그림 .</b> BERT의 input <br/>&nbsp;\n","  </td></tr>\n","</table>\n","\n","\n","- 2개의 문장을 BERT의 input 형태로 적절하게 변환해주는 함수 (`bert_encode`)\n","- BERT는 input으로 총 3가지를 필요로 함\n","  - `input_word_ids`: 2개 문장을 token으로 표현한 token sequence (맨 앞에는 [CLS] token 추가, 각 문장의 끝에는 [SEP] token 추가) + 최대 sequence 길이에 맞추어 padding token\n","  - `input_mask`: padding이 아닌 곳은 1, padding인 곳은 0\n","  - `input_type_ids`: 문장을 구분해주는 표시, 첫 번째 문장은 0, 두번째 문장은 1"]},{"cell_type":"code","metadata":{"id":"gLXuzV7I_QQ1","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596177199408,"user_tz":-540,"elapsed":75612,"user":{"displayName":"이예준","photoUrl":"","userId":"00427163814690958255"}}},"source":["# 문장을 tokenize하고 마지막에 [SEP] token 추가\n","def encode_sentence(s, tokenizer):\n","  tokens = list(tokenizer.tokenize(s))\n","  tokens.append('[SEP]')\n","  return tokenizer.convert_tokens_to_ids(tokens)\n","\n","def bert_encode(sentence1, sentence2, tokenizer, max_seq_length):\n","  sentence1 = encode_sentence(sentence1, tokenizer)\n","  sentence2 = encode_sentence(sentence2, tokenizer)\n","\n","  # 추가할 paddding token 개수\n","  num_pad = max_seq_length - 1 - len(sentence1) - len(sentence2)\n","\n","  # 전체 sequence의 길이는 max_seq_length 보다 작아야 함\n","  assert num_pad >= 0\n","\n","  pad = tf.zeros(num_pad, dtype=tf.int32)\n","\n","  # 제일 앞의 token은 [CLS], 다음은 첫번째 sentence, 두번째 sentence\n","  input_word_ids = tf.convert_to_tensor(tokenizer.convert_tokens_to_ids(['[CLS]']) + sentence1 + sentence2)\n","  # input mask는 padding이 아닌 부분은 1, padding은 0\n","  input_mask = tf.ones_like(input_word_ids)\n","\n","  # type mask는 첫번째 문장은 0, 두번째 문장은 1\n","  type_cls = tf.zeros(1, dtype=tf.int32)\n","  type_s1 = tf.zeros_like(sentence1)\n","  type_s2 = tf.ones_like(sentence2)\n","  input_type_ids = tf.concat([type_cls, type_s1, type_s2], axis=0)\n","\n","  # 모두 뒤에 zero padding 추가\n","  input_word_ids = tf.concat([input_word_ids, pad], axis=0)\n","  input_mask = tf.concat([input_mask, pad], axis=0)\n","  input_type_ids = tf.concat([input_type_ids, pad], axis=0)\n","  \n","  return input_word_ids, input_mask, input_type_ids"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"vFns0SjNK7f2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":357},"executionInfo":{"status":"ok","timestamp":1596177199409,"user_tz":-540,"elapsed":75593,"user":{"displayName":"이예준","photoUrl":"","userId":"00427163814690958255"}},"outputId":"719a6120-0a6c-43f2-c1ab-a9a298bf80cc"},"source":["sentence1 = '안녕 bert 모델!'\n","sentence2 = '나는 너를 사랑해'\n","\n","print(encode_sentence(sentence1, tokenizer))\n","print(encode_sentence(sentence2, tokenizer))\n","print(bert_encode(sentence1, sentence2, tokenizer, max_seq_length))"],"execution_count":12,"outputs":[{"output_type":"stream","text":["[9521, 118741, 10347, 10976, 9283, 118791, 106, 102]\n","[100585, 9004, 11513, 9405, 62200, 14523, 102]\n","(<tf.Tensor: shape=(64,), dtype=int32, numpy=\n","array([   101,   9521, 118741,  10347,  10976,   9283, 118791,    106,\n","          102, 100585,   9004,  11513,   9405,  62200,  14523,    102,\n","            0,      0,      0,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0,\n","            0,      0,      0,      0,      0,      0,      0,      0],\n","      dtype=int32)>, <tf.Tensor: shape=(64,), dtype=int32, numpy=\n","array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","      dtype=int32)>, <tf.Tensor: shape=(64,), dtype=int32, numpy=\n","array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","      dtype=int32)>)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"R7PQrRkGeCY3","colab_type":"text"},"source":["### 학습 데이터 예시"]},{"cell_type":"markdown","metadata":{"id":"Mhgc0sVcdSjP","colab_type":"text"},"source":["- 아래는 학습 데이터 예시\n","  - 두 개의 문장으로 각각의 데이터가 구성되고, 각 데이터에는 multi-class label이 달려있는 상황 가정"]},{"cell_type":"code","metadata":{"id":"FjDnUMx5LE33","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"executionInfo":{"status":"ok","timestamp":1596177199410,"user_tz":-540,"elapsed":75564,"user":{"displayName":"이예준","photoUrl":"","userId":"00427163814690958255"}},"outputId":"d5d402e0-153f-4bf0-a62d-ab396d3c7510"},"source":["# 예시 input 데이터\n","sentences = [['나는 1번째 데이터의 1번째 문장이다.', '여기는 1번째 데이터의 2번째 문장이다.'],\n","             ['두번째의 1번째 문장이다.','2번째 데이터의 두번째 문장이겠지.'],\n","             ['그리고 여기 3번째 데이터의 첫번째 문장','3번째 데이터의 2번째 문장일까?!'],\n","             ['여기는 마지막 데이터의 첫 문자임', 'last data, second sentence']]\n","\n","target_list = [1, 3, 4, 6]\n","\n","sentences"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[['나는 1번째 데이터의 1번째 문장이다.', '여기는 1번째 데이터의 2번째 문장이다.'],\n"," ['두번째의 1번째 문장이다.', '2번째 데이터의 두번째 문장이겠지.'],\n"," ['그리고 여기 3번째 데이터의 첫번째 문장', '3번째 데이터의 2번째 문장일까?!'],\n"," ['여기는 마지막 데이터의 첫 문자임', 'last data, second sentence']]"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"8O4uG116IfC6","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596177199411,"user_tz":-540,"elapsed":75556,"user":{"displayName":"이예준","photoUrl":"","userId":"00427163814690958255"}}},"source":["# bert_encode 함수를 이용하여, bert input으로 사용하기에 적절하게 전처리\n","ids_list, mask_list, type_list = list(), list(), list()\n","\n","\n","for s1, s2 in sentences:\n","  input_word_ids, input_mask, input_type_ids = bert_encode(s1, s2, tokenizer, max_seq_length)\n","  ids_list.append(input_word_ids)\n","  mask_list.append(input_mask)\n","  type_list.append(input_type_ids)"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"w3rNtKW4IwMd","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596177199411,"user_tz":-540,"elapsed":75535,"user":{"displayName":"이예준","photoUrl":"","userId":"00427163814690958255"}}},"source":["dataset = tf.data.Dataset.from_tensor_slices((ids_list, mask_list, type_list, target_list))"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"chzOoINUI-jX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":272},"executionInfo":{"status":"ok","timestamp":1596177199412,"user_tz":-540,"elapsed":75507,"user":{"displayName":"이예준","photoUrl":"","userId":"00427163814690958255"}},"outputId":"786759f1-993a-4ee8-8659-748d5a2e3604"},"source":["for input_word_ids, input_mask, input_type_ids, targets in dataset.take(1):\n","  print(input_word_ids)\n","  print(input_mask)\n","  print(input_type_ids)\n","  print(targets)"],"execution_count":16,"outputs":[{"output_type":"stream","text":["tf.Tensor(\n","[   101 100585    122  48506   9083  85297  10459    122  48506   9297\n","  55635  11903    119    102   9565  46216    122  48506   9083  85297\n","  10459    123  48506   9297  55635  11903    119    102      0      0\n","      0      0      0      0      0      0      0      0      0      0\n","      0      0      0      0      0      0      0      0      0      0\n","      0      0      0      0      0      0      0      0      0      0\n","      0      0      0      0], shape=(64,), dtype=int32)\n","tf.Tensor(\n","[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], shape=(64,), dtype=int32)\n","tf.Tensor(\n","[0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], shape=(64,), dtype=int32)\n","tf.Tensor(1, shape=(), dtype=int32)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"y68qKW4XR6ja","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":561},"executionInfo":{"status":"ok","timestamp":1596177199413,"user_tz":-540,"elapsed":75474,"user":{"displayName":"이예준","photoUrl":"","userId":"00427163814690958255"}},"outputId":"255d480d-4bad-46b7-97b3-c09fd85bfd49"},"source":["for batch in dataset.batch(2).take(1):\n","  print(batch)"],"execution_count":17,"outputs":[{"output_type":"stream","text":["(<tf.Tensor: shape=(2, 64), dtype=int32, numpy=\n","array([[   101, 100585,    122,  48506,   9083,  85297,  10459,    122,\n","         48506,   9297,  55635,  11903,    119,    102,   9565,  46216,\n","           122,  48506,   9083,  85297,  10459,    123,  48506,   9297,\n","         55635,  11903,    119,    102,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0],\n","       [   101,   9102,  48506,  10459,    122,  48506,   9297,  55635,\n","         11903,    119,    102,    123,  48506,   9083,  85297,  10459,\n","          9102,  48506,   9297,  55635, 118632,  12508,    119,    102,\n","             0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0,\n","             0,      0,      0,      0,      0,      0,      0,      0]],\n","      dtype=int32)>, <tf.Tensor: shape=(2, 64), dtype=int32, numpy=\n","array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n","      dtype=int32)>, <tf.Tensor: shape=(2, 64), dtype=int32, numpy=\n","array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n","      dtype=int32)>, <tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 3], dtype=int32)>)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ry8IhnFgQe4r","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"executionInfo":{"status":"ok","timestamp":1596177200613,"user_tz":-540,"elapsed":76633,"user":{"displayName":"이예준","photoUrl":"","userId":"00427163814690958255"}},"outputId":"5064f64c-152f-473e-df71-c35befa76c35"},"source":["for input_word_ids, input_mask, segment_ids, targets in dataset.batch(2):\n","  pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n","  print(pooled_output.shape)\n","  print(sequence_output.shape)"],"execution_count":18,"outputs":[{"output_type":"stream","text":["(2, 768)\n","(2, 64, 768)\n","(2, 768)\n","(2, 64, 768)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"HsM7t8nueIti","colab_type":"text"},"source":["## pre-trained BERT를 fine tuning 하기"]},{"cell_type":"markdown","metadata":{"id":"M750XKUWU0gl","colab_type":"text"},"source":["- fine-tuning을 위한 multi-class classifier로 만들기\n","  - [CLS] token은 sequence의 맨 앞에 있음\n","  - 맨 앞의 time step에 Dense layer를 이용하여 분류 logit 값을 계산"]},{"cell_type":"code","metadata":{"id":"mcYR3gHsUy4y","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1596177206267,"user_tz":-540,"elapsed":82284,"user":{"displayName":"이예준","photoUrl":"","userId":"00427163814690958255"}}},"source":["class BERT_classifier(tf.keras.Model):\n","    def __init__(self, num_class):\n","        super(BERT_classifier, self).__init__()\n","        self.bert_layer = hub.KerasLayer(hub_url_bert, trainable=True)\n","        self.dense = tf.keras.layers.Dense(num_class)\n","\n","    def call(self, input_word_ids, input_mask, segment_ids, training=False):\n","        pooled_output, sequence_output = self.bert_layer([input_word_ids, input_mask, segment_ids], training=training)\n","\n","        # [CLS] token이 sequence의 맨 앞에 있음\n","        cls_output = sequence_output[:, 0, :]\n","        logits = self.dense(cls_output)\n","        return logits\n","\n","num_class = 10\n","model = BERT_classifier(num_class)"],"execution_count":19,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pDA_pgECcv5a","colab_type":"text"},"source":["- multi-label classification인 경우, 아래와 같이 loss, optimizer 등을 선택하여 fine-tuning "]},{"cell_type":"code","metadata":{"id":"r-1kdvDKWORt","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1596177224696,"user_tz":-540,"elapsed":100663,"user":{"displayName":"이예준","photoUrl":"","userId":"00427163814690958255"}},"outputId":"6ff6a117-29f4-40cf-f92b-e19c7f106849"},"source":["loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","optimizer = tf.keras.optimizers.Adam()\n","training = True\n","\n","for input_word_ids, input_mask, segment_ids, targets in dataset.batch(2):\n","    with tf.GradientTape() as tape:\n","        logits  = model(input_word_ids, input_mask, segment_ids, training=training)\n","        loss = loss_object(targets, logits)\n","    gradients = tape.gradient(loss, model.trainable_variables)\n","    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n","\n","    print(logits.shape)"],"execution_count":20,"outputs":[{"output_type":"stream","text":["(2, 10)\n","(2, 10)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"m9eIuAzvdD6_","colab_type":"text"},"source":["- 실제 fine-tuning 하는 예시는 [tensorflow 튜토리얼](https://www.tensorflow.org/official_models/fine_tuning_bert)을 확인"]}]}