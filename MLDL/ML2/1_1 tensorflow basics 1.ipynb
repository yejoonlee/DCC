{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"1_1 tensorflow basics 1.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"qV40U7mLZkN4","colab_type":"text"},"source":["# Tensorflow 실습 1"]},{"cell_type":"markdown","metadata":{"id":"AqnaCOhoZsTc","colab_type":"text"},"source":["python의 딥러닝 라이브러리인 Tensorflow에 대해 알아보자."]},{"cell_type":"markdown","metadata":{"id":"h5X9HqjFaFVK","colab_type":"text"},"source":["### 기초 문법과 예제"]},{"cell_type":"markdown","metadata":{"id":"an123w8zaJKA","colab_type":"text"},"source":["Tensorflow 는 graph 로 연산을 나타내는 프로그래밍 시스템\n","- Graph 의 node 는 연산, 즉 operation(op) 을 수행\n","- Graph 의 edge 는 tensor 의 흐름을 나타냄\n","- Autograph 및 eager excution을 통해, \"define by run\" 방식이 default로 제공\n","텐서 (Tensor)\n","- Numpy 의 ndarray 와 비슷한 다차원 배열로, tensorflow 에서의 데이터를 표현하는 구조\n","- 그래프 내 operation 에서 tensor 가 전달됨\n","\n","케라스 (Keras)\n","- 기존에 존재하는 딥러닝 프레임워크로, 쉽게 이해할수 있는 코드로 구성되는 것이 특징\n","- Tensorflow에서 이를 적극적으로 활용하기로 결정하여, tf.keras라는 고수준 API로 다양한 layer를 사용할수 있도록 함\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ZVOIj2jKy77Y","colab_type":"text"},"source":["###Tensorflow의 간단한 연산 예시\n","- add, square, reduce_sum 등 간단한 함수들을 사용 가능\n","\n","- 연산의 결과는 tf.Tensor로 나오며, input 값의 형태에 따라 자동으로 dtype이 배정됨\n","\n","- 각각의 tf.Tensor는 shape와 dtype을 가지고 있음"]},{"cell_type":"code","metadata":{"id":"0KlWlN1wZTfo","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":119},"executionInfo":{"status":"ok","timestamp":1595568977744,"user_tz":-540,"elapsed":11312,"user":{"displayName":"이예준","photoUrl":"","userId":"00427163814690958255"}},"outputId":"cb903e04-674c-4cfd-d91c-220473ac5be8"},"source":["import tensorflow as tf\n","\n","# constant tensor\n","print(tf.constant([3, 7]))\n","\n","# 더하기\n","print(tf.add(1, 2))\n","\n","# vector 더하기\n","print(tf.add([1, 2], [3, 4]))\n","\n","# 제곱\n","print(tf.square(5.0))\n","\n","# 합\n","print(tf.reduce_sum([1, 2, 3]))\n","\n","# 각각 제곱 후 더하기 (연산자 오버로딩(overloading) 지원)\n","print(tf.square(2) + tf.square(3))"],"execution_count":1,"outputs":[{"output_type":"stream","text":["tf.Tensor([3 7], shape=(2,), dtype=int32)\n","tf.Tensor(3, shape=(), dtype=int32)\n","tf.Tensor([4 6], shape=(2,), dtype=int32)\n","tf.Tensor(25.0, shape=(), dtype=float32)\n","tf.Tensor(6, shape=(), dtype=int32)\n","tf.Tensor(13, shape=(), dtype=int32)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MVHBvIgZ30EF","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":136},"executionInfo":{"status":"ok","timestamp":1595569029007,"user_tz":-540,"elapsed":1443,"user":{"displayName":"이예준","photoUrl":"","userId":"00427163814690958255"}},"outputId":"d08ff758-41f5-4179-a9e8-a2d4ae5963fb"},"source":["# matrix 곱셈\n","a = tf.constant([[2],[3]])\n","b = tf.constant([[3,7]])\n","x = tf.matmul(a,b)\n","print(a.shape)\n","print(b.shape)\n","print(x)\n","\n","# tensor shape \n","print(x.shape)\n","\n","# tensor data type\n","print(x.dtype)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["(2, 1)\n","(1, 2)\n","tf.Tensor(\n","[[ 6 14]\n"," [ 9 21]], shape=(2, 2), dtype=int32)\n","(2, 2)\n","<dtype: 'int32'>\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wuuUF8A4aaGW","colab_type":"text"},"source":["###tensorflow와 numpy의 호환성\n","- list 또는 numpy array를 tf.Tensor로 변환할 때는, tf.convert_to_tensor 사용\n","- tensorflow 연산은 numpy array를 자동으로 tf.Tensor로 변환하여 사용\n","- 반대로, numpy 연산은 tf.Tensor를 numpy array로 변환함"]},{"cell_type":"code","metadata":{"id":"H-G7hjr-cXzJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":306},"executionInfo":{"status":"ok","timestamp":1595569033364,"user_tz":-540,"elapsed":1696,"user":{"displayName":"이예준","photoUrl":"","userId":"00427163814690958255"}},"outputId":"6770a8ff-0e5a-4de6-d234-428bc9945a6b"},"source":["import numpy as np\n","\n","ndarray = np.ones([3, 3])\n","# .convert_to_tensor 함수는 list, ndarray를 직접 텐서로 변환\n","print(tf.convert_to_tensor([[1,2],[3,4]], dtype=tf.float64))\n","print(tf.convert_to_tensor(ndarray, dtype=tf.int32))\n","\n","# 텐서플로 연산은 자동적으로 넘파이 배열을 텐서로 변환\n","tensor = tf.multiply(ndarray, 2)\n","print(tensor)\n","\n","# 그리고 넘파이 연산은 자동적으로 텐서를 넘파이 배열로 변환\n","print(np.add(tensor, 1))\n","\n","# .numpy() 메서드는 텐서를 넘파이 배열로 변환\n","print(tensor.numpy())"],"execution_count":3,"outputs":[{"output_type":"stream","text":["tf.Tensor(\n","[[1. 2.]\n"," [3. 4.]], shape=(2, 2), dtype=float64)\n","tf.Tensor(\n","[[1 1 1]\n"," [1 1 1]\n"," [1 1 1]], shape=(3, 3), dtype=int32)\n","tf.Tensor(\n","[[2. 2. 2.]\n"," [2. 2. 2.]\n"," [2. 2. 2.]], shape=(3, 3), dtype=float64)\n","[[3. 3. 3.]\n"," [3. 3. 3.]\n"," [3. 3. 3.]]\n","[[2. 2. 2.]\n"," [2. 2. 2.]\n"," [2. 2. 2.]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CXlsh8YbctVs","colab_type":"text"},"source":["### Variable (변수)\n","- 학습할 수 있는 parameter를 variable이라는 형태의 텐서로 표현함\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"d8ARMY0yctBR","colab_type":"code","colab":{}},"source":["# tf.Vaiable()을 통해, input으로 list, numpy array, tensor 등을 넣어주면, variable로 변환\n","my_variable = tf.Variable(tf.zeros([2, 3]))\n","print(my_variable)\n","\n","v = tf.Variable([[2,3],[3,4]])\n","print(v)\n","\n","# variable을 수식에 사용하면, 자동적으로 tf.Tensor로 변환되어 값을 표현\n","w = v + 1\n","print(w)\n","print(v)\n","\n","# variable에 .read_value()를 이용할 경우, 현재 변수 값을 명시적으로 읽어올 수 있음\n","print(v.read_value())\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L47Olvqv6JQa","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"CqL30baG56wr","colab_type":"code","colab":{}},"source":["# class에 tf.Module 등을 상속할 경우 .variables() 함수로 class가 보유한 변수 목록을 불러올 수 있음\n","class MyModuleOne(tf.Module):\n","    def __init__(self):\n","        self.v0 = tf.Variable(1.0)\n","        self.vs = [tf.Variable(x) for x in range(2)]\n","\n","m = MyModuleOne()\n","print(m.variables)\n","print(len(m.variables))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pi6Ywjt9Fvak","colab_type":"text"},"source":["### 자동 미분 (gradient tape의 사용법)\n","- gradient tape를 이용하면, variable에 대한 연산을 순서대로 모두 저장하고 자동으로 gradient를 계산하는 것이 가능\n","- with 구문 안의 연산을 tape에 저장하면, tape.gradient(target, sources)로 우리가 원하는 형태의 미분 값을 계산할 수 있음"]},{"cell_type":"code","metadata":{"id":"h_9xX9DRFvKm","colab_type":"code","colab":{}},"source":["x = tf.Variable(3.0)\n","\n","with tf.GradientTape() as tape:\n","  y = x**2\n","\n","# dy/dy = 2x \n","dy_dx = tape.gradient(y, x)\n","dy_dx.numpy()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7qMpQSrbH9ff","colab_type":"text"},"source":["- 위의 미분 값은 scalar 형태이지만, gradient는 tensor형태도 될수 있음\n","- sources 부분에 list를 넣으면 list가 출력되고, dictionary를 넣으면 dictionary 형태로 출력됨"]},{"cell_type":"code","metadata":{"id":"YvrCd9VLIPZ_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1595569047491,"user_tz":-540,"elapsed":3062,"user":{"displayName":"이예준","photoUrl":"","userId":"00427163814690958255"}},"outputId":"ef73001a-1114-4e3a-bc23-3677878b77bc"},"source":["w = tf.Variable(tf.random.normal((3, 2)), name='w')\n","b = tf.Variable(tf.zeros(2, dtype=tf.float32), name='b')\n","x = tf.constant([[1., 2., 3.]])\n","\n","with tf.GradientTape(persistent=True) as tape:\n","  y = x @ w + b  ## 참고: tf.matmul(x, w) 와 x @ w 동일함\n","  loss = tf.reduce_mean(y**2)\n","\n","[dl_dw, dl_db] = tape.gradient(loss, [w, b])\n","print(w.shape)\n","print(dl_dw.shape)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["(3, 2)\n","(3, 2)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"imtswIa9IIMR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"executionInfo":{"status":"ok","timestamp":1595569051864,"user_tz":-540,"elapsed":1871,"user":{"displayName":"이예준","photoUrl":"","userId":"00427163814690958255"}},"outputId":"c7c05928-d8b4-4f99-e9c0-a83f562600a3"},"source":["my_vars = {\n","    'w': w,\n","    'b': b\n","}\n","grad = tape.gradient(loss, my_vars)\n","print(grad)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["{'w': <tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n","array([[-0.26585382,  6.12057   ],\n","       [-0.53170764, 12.24114   ],\n","       [-0.79756147, 18.36171   ]], dtype=float32)>, 'b': <tf.Tensor: shape=(2,), dtype=float32, numpy=array([-0.26585382,  6.12057   ], dtype=float32)>}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KehtZou9dHXd","colab_type":"text"},"source":["###Device\n","- tensorflow로 설치할 경우 CPU만 사용 가능\n","- tensorflow-gpu로 설치할 경우 CPU, GPU 모두 사용 가능\n","- with tf.device(\"디바이스 종류 및 번호\")\n","위의 코드를 통해, 연산이 실행되는 device를 지정할 수 있음\n","- 같은 연산일지라도, CPU보다는 GPU에서 훨씬 빠른 속도를 보임\n","- Colab에서 GPU 설정하기: 런타임 -> 런타임 유형 변경 -> 하드웨어 가속기 GPU로 세팅"]},{"cell_type":"code","metadata":{"id":"uK1Y8xsEdq4F","colab_type":"code","colab":{}},"source":["import time\n","\n","def time_matmul(x):\n","  start = time.time()\n","  for loop in range(100):\n","    tf.matmul(x, x)\n","\n","  result = time.time()-start\n","  print(\"100 loops: {:0.2f}ms\".format(1000*result))\n","\n","# CPU에서 강제 실행합니다.\n","print(tf.config.list_physical_devices('CPU'))\n","if tf.config.list_physical_devices('CPU'):\n","  print(\"On CPU:\")\n","  with tf.device(\"CPU:0\"):\n","    x = tf.random.uniform([1000, 1000])\n","    assert x.device.endswith(\"CPU:0\")\n","    time_matmul(x)\n","\n","# GPU #0가 이용가능시 GPU #0에서 강제 실행합니다.\n","print(tf.config.list_physical_devices('GPU'))\n","if tf.config.list_physical_devices('GPU'):\n","  print(\"On GPU:\")\n","  with tf.device(\"GPU:0\"): # Or GPU:1 for the 2nd GPU, GPU:2 for the 3rd etc.\n","    x = tf.random.uniform([1000, 1000])\n","    assert x.device.endswith(\"GPU:0\")\n","    time_matmul(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3Ag_cWMAT4nM","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","\n","var = tf.constant([2,3])\n","\n"],"execution_count":null,"outputs":[]}]}