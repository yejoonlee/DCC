{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"6_Ensemble_Full_Version.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMzm1CIqpIxl0c0U6AnwSqP"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"eiory5W_FV1x","colab_type":"text"},"source":["# 앙상블 실습"]},{"cell_type":"markdown","metadata":{"id":"GWvL6HnCFXf3","colab_type":"text"},"source":["수업 시간에 test data에 대한 예측 정확도를 높이는 방법으로 앙상블이 사용된다고 설명한 바 있다. 앙상블을 한 마디로 표현하면 **집단 지성**으로, 여러 가지 모델을 결합해 최종 예측을 진행한다. 주로 variance를 줄이는 데 큰 역할을 하는 앙상블은 대부분의 task에서 성능이 매우 뛰어난 방법론이라 알려져 있다. "]},{"cell_type":"markdown","metadata":{"id":"xRbgGfAmGOtm","colab_type":"text"},"source":["### 합의 기반 결합"]},{"cell_type":"markdown","metadata":{"id":"5pV206rPGSBX","colab_type":"text"},"source":["앙상블 방법론은 머신 러닝 모델의 결합 방법에 따라 합의 기반 결합과 학습 기반 결합으로 나눌 수 있다. 이번 실습에서는 합의 기반 결합에 대해 알아보자."]},{"cell_type":"markdown","metadata":{"id":"YFRfA_dWQ6_E","colab_type":"text"},"source":["![](https://miro.medium.com/max/700/1*I7NsQXwyR36XK62s1iDNzQ.png)"]},{"cell_type":"markdown","metadata":{"id":"K_7VFBYbO4QW","colab_type":"text"},"source":["합의 기반 결합은 쉽게 말해 **다수결**을 이용하는 것이다. 단순한 예로, 70%의 정확도를 가지는 모델이 5개 있다고 하자. 만약 이들이 서로 독립이라면, 이들의 결과로 다수결 투표를 진행했을 때의 정확도는 83.7% 에 달한다."]},{"cell_type":"markdown","metadata":{"id":"my_Uua7cPtUy","colab_type":"text"},"source":["이 때 중요한 것은 모델들이 서로 **독립**이어야 한다는 것이다. 독립이 아니라 똑같은 모델 5개이면 다수결을 해도 정확도는 70 %에서 변하지 않는다. 따라서 합의 기반 결합에서는 서로 독립인, 혹은 다양한 특징을 갖는 모델들을 생성해야 한다는 것이 매우 중요하다.  "]},{"cell_type":"markdown","metadata":{"id":"9MakQDcyNnJb","colab_type":"text"},"source":["### 합의 기반 결합 - Voting"]},{"cell_type":"markdown","metadata":{"id":"rHK_xnmWNpqu","colab_type":"text"},"source":["Voting 기법은 말 그대로 여러 개의 모델에게 정답이 무엇인지 투표시키는 방식을 말한다. 이 때 각 모델은 서로 다른 방법론을 통해 만들어진다. python의 scikit-learn 패키지의 [VotingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html?highlight=votingclassifier#sklearn.ensemble.VotingClassifier) 를 이용해 기본적인 voting 방법론을 직접 실행해보자."]},{"cell_type":"markdown","metadata":{"id":"4twcgA9HJSpz","colab_type":"text"},"source":["[Covertype](https://archive.ics.uci.edu/ml/datasets/Covertype) 데이터셋을 사용해 보자. Scikit-learn의 [fetch_covtype](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_covtype.html#sklearn.datasets.fetch_covtype) 함수로 데이터를 받아올 수 있다."]},{"cell_type":"code","metadata":{"id":"GnRYGOS1SWVO","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"status":"ok","timestamp":1595266606778,"user_tz":-540,"elapsed":2084,"user":{"displayName":"허완","photoUrl":"","userId":"07707040825991561570"}},"outputId":"7da537ef-1505-4b37-f102-3bd3bf79cd72"},"source":["from sklearn.datasets import fetch_covtype\n","x, y = fetch_covtype(return_X_y=True)\n","print(x.shape)\n","print(y.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(581012, 54)\n","(581012,)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BS-GdghbSQzX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1595266606779,"user_tz":-540,"elapsed":1871,"user":{"displayName":"허완","photoUrl":"","userId":"07707040825991561570"}},"outputId":"89d1929b-7bae-4da1-870d-8f7a99f36e4b"},"source":["from sklearn.model_selection import train_test_split\n","\n","# 데이터 나누기\n","num_use_data = 2000\n","x = x[:num_use_data, :]\n","y = y[:num_use_data]\n","\n","x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1)\n","\n","print(x_train.shape, x_test.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(1600, 54) (400, 54)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jdJeVc-DXcrq","colab_type":"text"},"source":["VotingClassifier를 학습하기 위해, 수업에서 배웠던 세 가지 분류기들을 이용해 보자. 먼저 필요한 모델들을 import 한 후 각각 생성하자."]},{"cell_type":"code","metadata":{"id":"isDZPeJ0JSaC","colab_type":"code","colab":{}},"source":["from sklearn.ensemble import VotingClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.svm import SVC\n","from sklearn.tree import DecisionTreeClassifier"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tJCcrigoGOS8","colab_type":"code","colab":{}},"source":["# 모델 객체 생성하기\n","log_clf = LogisticRegression()\n","dt_clf = DecisionTreeClassifier(random_state=1)\n","svm_clf = SVC()\n","\n","voting_clf = VotingClassifier(\n","    estimators=[('lr', log_clf), \n","                ('dt', dt_clf), \n","                ('svc', svm_clf)])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1Cuo7quGTczW","colab_type":"text"},"source":["각 모델들을 학습시킨 후 정확도를 비교해 보자."]},{"cell_type":"code","metadata":{"id":"gQiW4df2FVkt","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":89},"executionInfo":{"status":"ok","timestamp":1595266613331,"user_tz":-540,"elapsed":1804,"user":{"displayName":"허완","photoUrl":"","userId":"07707040825991561570"}},"outputId":"70d5007d-8a92-4d58-e8c6-aba7f44745e7"},"source":["# 각 모델 학습 및 정확도 확인하기\n","from sklearn.metrics import accuracy_score\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","voting_clf.fit(x_train, y_train)\n","pred_test = voting_clf.predict(x_test)\n","print(f'voting accuracy: {accuracy_score(y_test, pred_test)}')\n","\n","for name, estimator in voting_clf.named_estimators_.items():\n","  estimator.fit(x_train, y_train)\n","  pred_test_estim = estimator.predict(x_test)\n","  print(f'{name}: {accuracy_score(y_test, pred_test_estim)}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["voting accuracy: 0.69\n","lr: 0.615\n","dt: 0.7475\n","svc: 0.605\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"sZpAkmT5WXEX","colab_type":"text"},"source":["VotingClassifier의 파라미터 중 하나인 voting은 다수결의 방식을 뜻한다. Default인 'hard'는 각 모델이 표를 하나씩 행사하는 방식이며, 'soft'는 각 모델의 분류 확률을 모두 더해 가장 높은 확률로 예측하는 방식이다. 일반적으로 soft voting 방식이 더 효과가 좋다고 알려져 있다. 이를 구현하고 위의 hard voting 방식과 비교해 보자."]},{"cell_type":"code","metadata":{"id":"Kh8eItRS65ah","colab_type":"code","colab":{}},"source":["# soft voting 방식으로 모델 생성하기\n","log_clf_2 = LogisticRegression()\n","dt_clf_2 = DecisionTreeClassifier(random_state=1)\n","svm_clf_2 = SVC(probability=True)\n","\n","voting_clf_2 = VotingClassifier(\n","    estimators=[('lr', log_clf_2), \n","                ('dt', dt_clf_2), \n","                ('svc', svm_clf_2)],\n","                voting='soft')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xb5fgQ1tSuwz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":89},"executionInfo":{"status":"ok","timestamp":1595266667363,"user_tz":-540,"elapsed":3944,"user":{"displayName":"허완","photoUrl":"","userId":"07707040825991561570"}},"outputId":"57e619e2-f062-4e5d-8046-eef26e8784bc"},"source":["# soft voting 방식으로 생성한 모델 학습하기\n","voting_clf_2.fit(x_train, y_train)\n","pred_test_2 = voting_clf_2.predict(x_test)\n","print(f'soft voting accuracy: {accuracy_score(y_test, pred_test_2)}')\n","\n","for name, estimator in voting_clf_2.named_estimators_.items():\n","  estimator.fit(x_train, y_train)\n","  pred_test_estim = estimator.predict(x_test)\n","  print(f'{name}: {accuracy_score(y_test, pred_test_estim)}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["soft voting accuracy: 0.765\n","lr: 0.615\n","dt: 0.7475\n","svc: 0.605\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yKnFe32_Zg33","colab_type":"text"},"source":["### 합의 기반 결합 - Bagging"]},{"cell_type":"markdown","metadata":{"id":"3wkJqsm6ZjQC","colab_type":"text"},"source":["Bagging 기법은 voting과 달리 하나의 머신 러닝 방법론을 사용하지만, 주어진 학습 데이터나 변수를 모두 사용하지 않고 임의로 추출하여 다양한 모델을 학습시킨 후 이들의 합의 기반 결합을 진행한다. "]},{"cell_type":"markdown","metadata":{"id":"z21Kfn7Qabxs","colab_type":"text"},"source":["![](https://upload.wikimedia.org/wikipedia/commons/thumb/c/c8/Ensemble_Bagging.svg/512px-Ensemble_Bagging.svg.png)"]},{"cell_type":"markdown","metadata":{"id":"pbbfOc2KaeJs","colab_type":"text"},"source":["Bagging의 종류는 각 모델의 학습용 데이터를 어떻게 구성하느냐에 따라 아래의 네 가지로 구분지을 수 있다.\n","\n","\n","* Pasting: 같은 데이터를 중복해서 추출하지 않음\n","* Bagging: 같은 데이터 샘플을 중복해서 추출\n","* Random Subspace: 데이터가 아니라 주어진 독립 변수 중 일부를 랜덤 추출\n","* Random Patches: 데이터와 독립 변수 모두 랜덤 추출해서 사용\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"GKwFpPJ40yda","colab_type":"text"},"source":["![alt text](https://datascienceschool.net/upfiles/677f18a151c64e5daa9c28f6cb564808.png)"]},{"cell_type":"markdown","metadata":{"id":"mQmzAMrofzLy","colab_type":"text"},"source":["먼저 일반적인 의사결정 나무 분류기를 학습한 후, [BaggingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html?highlight=bagging#sklearn.ensemble.BaggingClassifier)를 이용해 bagging을 적용한 의사결정 나무 분류기를 학습해 보자."]},{"cell_type":"code","metadata":{"id":"dJrSZbGKc7ev","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1595266670910,"user_tz":-540,"elapsed":706,"user":{"displayName":"허완","photoUrl":"","userId":"07707040825991561570"}},"outputId":"aeefe650-992f-400b-ff5e-89600b5b68e8"},"source":["tree_clf = DecisionTreeClassifier(random_state=1)\n","tree_clf.fit(x_train, y_train)\n","pred_test = tree_clf.predict(x_test)\n","print(f'accuracy: {accuracy_score(y_test, pred_test)}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["accuracy: 0.7475\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QVkQn1PYdH8K","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1595266672887,"user_tz":-540,"elapsed":2502,"user":{"displayName":"허완","photoUrl":"","userId":"07707040825991561570"}},"outputId":"a0483457-799d-4c32-cc7d-941b80c68f32"},"source":["# BaggingClassifier를 사용해 의사결정 나무 분류기 학습 및 정확도 계산하기\n","from sklearn.ensemble import BaggingClassifier\n","\n","bag_clf = BaggingClassifier(\n","    DecisionTreeClassifier(random_state=1), n_estimators=300,\n","    max_samples=500, random_state=1)\n","bag_clf.fit(x_train, y_train)\n","\n","pred_test_bag = bag_clf.predict(x_test)\n","print(f'accuracy: {accuracy_score(y_test, pred_test_bag)}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["accuracy: 0.845\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Zmdh8bfOh9ye","colab_type":"text"},"source":["의사결정 나무에서 배운 재귀적 분기에 대해 떠올려보자. 분기를 할 후보들을 각 독립 변수들로 정렬한 후에 최적의 분기점을 불순도 함수를 이용해 찾는다고 배웠다. 이 때, 모든 독립 변수를 사용하는 것이 아니라 **일부**만을 골라 분기 후보를 찾는다면, 더 빠른 계산이 가능하다. 이것이 바로 위에서 말한 random subspace 혹은 random patches의 개념을 적용한 것이다."]},{"cell_type":"markdown","metadata":{"id":"DVC7qophikYR","colab_type":"text"},"source":["즉, random patches를 의사결정 나무에 사용한 것이 바로 RandomForest 이다. Scikit-learn 에서는 [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html?highlight=randomforest#sklearn.ensemble.RandomForestClassifier)에서 이 모델을 제공하고 있으니, 이를 사용해서 분류해 보자."]},{"cell_type":"code","metadata":{"id":"_h6fwI5meq7s","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1595266694729,"user_tz":-540,"elapsed":1785,"user":{"displayName":"허완","photoUrl":"","userId":"07707040825991561570"}},"outputId":"90b12228-ab75-48f8-cb07-7933c4749345"},"source":["# RandomForestClassifier를 사용해 분류기 학습 및 정확도 계산하기\n","from sklearn.ensemble import RandomForestClassifier\n","\n","rf_clf = RandomForestClassifier(n_estimators=300, random_state=1)\n","rf_clf.fit(x_train, y_train)\n","pred_test_rf = rf_clf.predict(x_test)\n","print(f'accuracy: {accuracy_score(y_test, pred_test_rf)}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["accuracy: 0.8425\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"AtN3ccdf8A0e","colab_type":"text"},"source":["GridSearchCV를 이용해 최고의 파라미터 조합을 찾아보자."]},{"cell_type":"code","metadata":{"id":"WsJB50RMpQDz","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import GridSearchCV"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CnFaFPChpSQ-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":431},"executionInfo":{"status":"ok","timestamp":1595267413207,"user_tz":-540,"elapsed":104470,"user":{"displayName":"허완","photoUrl":"","userId":"07707040825991561570"}},"outputId":"1561244c-71e4-483d-a9cc-5b48dac19c57"},"source":["# GridSearchCV를 이용해 최적의 조합을 찾고 그 때의 정확도 계산하기\n","n_estimators = [100, 200, 300]\n","max_featrues = ['auto', 0.2, 0.5]\n","min_samples_leaf = [1, 5]\n","max_depth = [None, 10]\n","max_samples = [1000]\n","\n","param_grid = {'n_estimators' : n_estimators, 'max_features': max_featrues, \n","              'min_samples_leaf': min_samples_leaf, 'max_depth': max_depth, \n","              'max_samples': max_samples}\n","\n","rf_grid = GridSearchCV(RandomForestClassifier(), param_grid=param_grid, cv=4, scoring='accuracy')\n","\n","rf_grid.fit(x_train, y_train)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["GridSearchCV(cv=4, error_score=nan,\n","             estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n","                                              class_weight=None,\n","                                              criterion='gini', max_depth=None,\n","                                              max_features='auto',\n","                                              max_leaf_nodes=None,\n","                                              max_samples=None,\n","                                              min_impurity_decrease=0.0,\n","                                              min_impurity_split=None,\n","                                              min_samples_leaf=1,\n","                                              min_samples_split=2,\n","                                              min_weight_fraction_leaf=0.0,\n","                                              n_estimators=100, n_jobs=None,\n","                                              oob_score=False,\n","                                              random_state=None, verbose=0,\n","                                              warm_start=False),\n","             iid='deprecated', n_jobs=None,\n","             param_grid={'max_depth': [None, 10],\n","                         'max_features': ['auto', 0.2, 0.5],\n","                         'max_samples': [1000], 'min_samples_leaf': [1, 5],\n","                         'n_estimators': [100, 200, 300]},\n","             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n","             scoring='accuracy', verbose=0)"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"76Mpetatpql_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"status":"ok","timestamp":1595267413209,"user_tz":-540,"elapsed":104142,"user":{"displayName":"허완","photoUrl":"","userId":"07707040825991561570"}},"outputId":"452302b0-efc0-4312-c2eb-7ff3228c34b7"},"source":["print(rf_grid.best_params_)\n","pred_test_rf_grid = rf_grid.predict(x_test)\n","print(f'accuracy: {accuracy_score(y_test, pred_test_rf_grid)}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'max_depth': None, 'max_features': 0.5, 'max_samples': 1000, 'min_samples_leaf': 1, 'n_estimators': 100}\n","accuracy: 0.86\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dVA5l3oVV13M","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}